"""
ORQUESTRADOR DE SCRAPERS COM PLAYWRIGHT
========================================

Sistema centralizado para orquestrar a execu√ß√£o de todos os scrapers
usando Playwright. Gerencia rate limiting, agendamento, monitoramento
e consolida√ß√£o de dados.

Autor: Sistema de Coleta de Dados
Data: 2025-08-14
Vers√£o: 1.0
"""

import asyncio
import logging
import time
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
import signal
import sys

# Importar scrapers
from apis.playwright_base import PlaywrightBaseScraper
from apis.fbref.playwright_scraper import FBRefPlaywrightScraper
from apis.sofascore.playwright_scraper import SofaScorePlaywrightScraper
from apis.scraper_config import ScraperConfig

logger = logging.getLogger(__name__)

class OrquestradorScrapers:
    """
    Orquestrador principal para todos os scrapers.
    
    Funcionalidades:
    - Execu√ß√£o coordenada de m√∫ltiplos scrapers
    - Rate limiting inteligente
    - Agendamento autom√°tico
    - Monitoramento de sa√∫de
    - Consolida√ß√£o de dados
    - Tratamento de erros
    """
    
    def __init__(self, environment: str = "dev"):
        """
        Inicializa o orquestrador.
        
        Args:
            environment: Ambiente de execu√ß√£o ('dev' ou 'prod')
        """
        self.environment = environment
        self.config = ScraperConfig()
        self.scrapers = {}
        self.running = False
        self.stats = {
            "started_at": None,
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "last_success": None,
            "last_error": None
        }
        
        # Configurar logging
        self._setup_logging()
        
        # Configurar signal handlers
        self._setup_signal_handlers()
        
        logger.info(f"üé≠ OrquestradorScrapers inicializado para ambiente: {environment}")
    
    def _setup_logging(self):
        """Configura sistema de logging."""
        log_config = self.config.LOGGING_CONFIG
        
        # Criar diret√≥rio de logs
        log_file = Path(log_config["file"])
        log_file.parent.mkdir(parents=True, exist_ok=True)
        
        # Configurar logging
        logging.basicConfig(
            level=getattr(logging, log_config["level"]),
            format=log_config["format"],
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler(sys.stdout)
            ]
        )
    
    def _setup_signal_handlers(self):
        """Configura handlers para sinais do sistema."""
        def signal_handler(signum, frame):
            logger.info(f"üì° Sinal {signum} recebido, parando orquestrador...")
            self.stop()
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
    
    async def start(self):
        """Inicia o orquestrador."""
        try:
            logger.info("üöÄ Iniciando orquestrador de scrapers...")
            
            # Validar configura√ß√µes
            if not self.config.validate_config():
                raise Exception("Configura√ß√µes inv√°lidas")
            
            # Criar diret√≥rios necess√°rios
            self.config.create_directories()
            
            # Inicializar scrapers
            await self._initialize_scrapers()
            
            self.running = True
            self.stats["started_at"] = datetime.now().isoformat()
            
            logger.info("‚úÖ Orquestrador iniciado com sucesso")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao iniciar orquestrador: {e}")
            raise
    
    async def stop(self):
        """Para o orquestrador."""
        try:
            logger.info("üõë Parando orquestrador...")
            
            self.running = False
            
            # Parar todos os scrapers
            for scraper_name, scraper in self.scrapers.items():
                try:
                    await scraper.stop()
                    logger.info(f"‚úÖ {scraper_name} parado")
                except Exception as e:
                    logger.error(f"‚ùå Erro ao parar {scraper_name}: {e}")
            
            # Salvar estat√≠sticas finais
            await self._save_final_stats()
            
            logger.info("‚úÖ Orquestrador parado com sucesso")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao parar orquestrador: {e}")
    
    async def _initialize_scrapers(self):
        """Inicializa todos os scrapers."""
        try:
            logger.info("üîß Inicializando scrapers...")
            
            # Configura√ß√£o base do Playwright
            playwright_config = get_config_for_environment(self.environment)
            
            # Inicializar FBRef scraper
            self.scrapers["FBRef"] = FBRefPlaywrightScraper(**playwright_config)
            logger.info("‚úÖ FBRef scraper inicializado")
            
            # Inicializar SofaScore scraper
            self.scrapers["SofaScore"] = SofaScorePlaywrightScraper(**playwright_config)
            logger.info("‚úÖ SofaScore scraper inicializado")
            
            # Adicionar mais scrapers aqui conforme necess√°rio
            
            logger.info(f"‚úÖ {len(self.scrapers)} scrapers inicializados")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao inicializar scrapers: {e}")
            raise
    
    async def run_collection_cycle(self, cycle_name: str = "default"):
        """
        Executa um ciclo completo de coleta.
        
        Args:
            cycle_name: Nome do ciclo para identifica√ß√£o
        """
        try:
            logger.info(f"üîÑ Iniciando ciclo de coleta: {cycle_name}")
            
            cycle_start = time.time()
            cycle_stats = {
                "cycle_name": cycle_name,
                "started_at": datetime.now().isoformat(),
                "scrapers": {},
                "total_data_collected": 0
            }
            
            # Executar FBRef scraper
            if "FBRef" in self.scrapers:
                logger.info("üèÜ Executando FBRef scraper...")
                fbref_stats = await self._run_fbref_scraper()
                cycle_stats["scrapers"]["FBRef"] = fbref_stats
                cycle_stats["total_data_collected"] += fbref_stats.get("data_count", 0)
            
            # Executar SofaScore scraper
            if "SofaScore" in self.scrapers:
                logger.info("‚öΩ Executando SofaScore scraper...")
                sofascore_stats = await self._run_sofascore_scraper()
                cycle_stats["scrapers"]["SofaScore"] = sofascore_stats
                cycle_stats["total_data_collected"] += sofascore_stats.get("data_count", 0)
            
            # Rate limiting entre scrapers
            await asyncio.sleep(self.config.RATE_LIMITING["global_delay"])
            
            # Finalizar ciclo
            cycle_duration = time.time() - cycle_start
            cycle_stats["duration"] = cycle_duration
            cycle_stats["finished_at"] = datetime.now().isoformat()
            
            # Salvar estat√≠sticas do ciclo
            await self._save_cycle_stats(cycle_stats)
            
            logger.info(f"‚úÖ Ciclo {cycle_name} conclu√≠do em {cycle_duration:.2f}s")
            logger.info(f"üìä Total de dados coletados: {cycle_stats['total_data_collected']}")
            
            return cycle_stats
            
        except Exception as e:
            logger.error(f"‚ùå Erro no ciclo de coleta {cycle_name}: {e}")
            return None
    
    async def _run_fbref_scraper(self) -> Dict[str, Any]:
        """Executa o scraper do FBRef."""
        try:
            scraper = self.scrapers["FBRef"]
            
            # Configura√ß√£o espec√≠fica do FBRef
            fbref_config = self.config.get_fbref_config()
            
            # Coletar dados
            data = await scraper.collect_all_data(fbref_config["competitions"])
            
            # Salvar dados
            if data:
                filename = f"fbref_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                filepath = await scraper.save_data(data, filename)
                
                stats = {
                    "status": "success",
                    "data_count": len(data.get("matches", [])),
                    "competitions_processed": len(data.get("competitions", [])),
                    "file_saved": filepath,
                    "collected_at": datetime.now().isoformat()
                }
                
                self.stats["successful_requests"] += 1
                self.stats["last_success"] = datetime.now().isoformat()
                
                return stats
            else:
                stats = {
                    "status": "no_data",
                    "data_count": 0,
                    "competitions_processed": 0,
                    "file_saved": None,
                    "collected_at": datetime.now().isoformat()
                }
                
                return stats
                
        except Exception as e:
            logger.error(f"‚ùå Erro no scraper FBRef: {e}")
            
            self.stats["failed_requests"] += 1
            self.stats["last_error"] = datetime.now().isoformat()
            
            return {
                "status": "error",
                "error": str(e),
                "data_count": 0,
                "competitions_processed": 0,
                "file_saved": None,
                "collected_at": datetime.now().isoformat()
            }
    
    async def _run_sofascore_scraper(self) -> Dict[str, Any]:
        """Executa o scraper do SofaScore."""
        try:
            scraper = self.scrapers["SofaScore"]
            
            # Configura√ß√£o espec√≠fica do SofaScore
            sofascore_config = self.config.get_sofascore_config()
            
            # URLs organizadas por tipo
            urls = {
                "live": [f"{sofascore_config['base_url']}/football/live"],
                "teams": sofascore_config["teams"][:3],  # Limitar a 3 times por ciclo
                "tournaments": sofascore_config["tournaments"][:2]  # Limitar a 2 torneios por ciclo
            }
            
            # Coletar dados
            data = await scraper.collect_all_data(urls)
            
            # Salvar dados
            if data:
                filename = f"sofascore_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                filepath = await scraper.save_data(data, filename)
                
                stats = {
                    "status": "success",
                    "data_count": (
                        len(data.get("live_matches", [])) +
                        len(data.get("team_matches", [])) +
                        len(data.get("tournament_matches", []))
                    ),
                    "live_matches": len(data.get("live_matches", [])),
                    "team_matches": len(data.get("team_matches", [])),
                    "tournament_matches": len(data.get("tournament_matches", [])),
                    "file_saved": filepath,
                    "collected_at": datetime.now().isoformat()
                }
                
                self.stats["successful_requests"] += 1
                self.stats["last_success"] = datetime.now().isoformat()
                
                return stats
            else:
                stats = {
                    "status": "no_data",
                    "data_count": 0,
                    "live_matches": 0,
                    "team_matches": 0,
                    "tournament_matches": 0,
                    "file_saved": None,
                    "collected_at": datetime.now().isoformat()
                }
                
                return stats
                
        except Exception as e:
            logger.error(f"‚ùå Erro no scraper SofaScore: {e}")
            
            self.stats["failed_requests"] += 1
            self.stats["last_error"] = datetime.now().isoformat()
            
            return {
                "status": "error",
                "error": str(e),
                "data_count": 0,
                "live_matches": 0,
                "team_matches": 0,
                "tournament_matches": 0,
                "file_saved": None,
                "collected_at": datetime.now().isoformat()
            }
    
    async def _save_cycle_stats(self, cycle_stats: Dict[str, Any]):
        """Salva estat√≠sticas de um ciclo."""
        try:
            stats_dir = Path("stats")
            stats_dir.mkdir(exist_ok=True)
            
            filename = f"cycle_{cycle_stats['cycle_name']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            filepath = stats_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(cycle_stats, f, indent=2, ensure_ascii=False)
            
            logger.info(f"üíæ Estat√≠sticas do ciclo salvas: {filepath}")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar estat√≠sticas do ciclo: {e}")
    
    async def _save_final_stats(self):
        """Salva estat√≠sticas finais do orquestrador."""
        try:
            stats_dir = Path("stats")
            stats_dir.mkdir(exist_ok=True)
            
            # Adicionar estat√≠sticas finais
            self.stats["finished_at"] = datetime.now().isoformat()
            if self.stats["started_at"]:
                start_time = datetime.fromisoformat(self.stats["started_at"])
                end_time = datetime.now()
                self.stats["total_duration"] = (end_time - start_time).total_seconds()
            
            filename = f"orchestrator_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            filepath = stats_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.stats, f, indent=2, ensure_ascii=False)
            
            logger.info(f"üíæ Estat√≠sticas finais salvas: {filepath}")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar estat√≠sticas finais: {e}")
    
    async def run_scheduled_collection(self, interval_minutes: int = 60):
        """
        Executa coleta agendada.
        
        Args:
            interval_minutes: Intervalo entre coletas em minutos
        """
        try:
            logger.info(f"‚è∞ Iniciando coleta agendada a cada {interval_minutes} minutos")
            
            cycle_count = 0
            
            while self.running:
                try:
                    cycle_count += 1
                    cycle_name = f"scheduled_{cycle_count:04d}"
                    
                    logger.info(f"üîÑ Executando ciclo agendado {cycle_count}")
                    
                    # Executar ciclo de coleta
                    cycle_stats = await self.run_collection_cycle(cycle_name)
                    
                    if cycle_stats:
                        logger.info(f"‚úÖ Ciclo {cycle_count} conclu√≠do com sucesso")
                    else:
                        logger.warning(f"‚ö†Ô∏è Ciclo {cycle_count} falhou")
                    
                    # Aguardar pr√≥ximo ciclo
                    if self.running:
                        logger.info(f"‚è≥ Aguardando {interval_minutes} minutos para pr√≥ximo ciclo...")
                        await asyncio.sleep(interval_minutes * 60)
                    
                except asyncio.CancelledError:
                    logger.info("üõë Coleta agendada cancelada")
                    break
                except Exception as e:
                    logger.error(f"‚ùå Erro no ciclo agendado {cycle_count}: {e}")
                    
                    # Aguardar antes de tentar novamente
                    if self.running:
                        await asyncio.sleep(5 * 60)  # 5 minutos
            
            logger.info("üõë Coleta agendada parada")
            
        except Exception as e:
            logger.error(f"‚ùå Erro na coleta agendada: {e}")
    
    def get_health_status(self) -> Dict[str, Any]:
        """Retorna status de sa√∫de do orquestrador."""
        try:
            current_time = datetime.now()
            
            # Calcular tempo desde √∫ltima execu√ß√£o
            last_success_age = None
            if self.stats["last_success"]:
                last_success = datetime.fromisoformat(self.stats["last_success"])
                last_success_age = (current_time - last_success).total_seconds()
            
            last_error_age = None
            if self.stats["last_error"]:
                last_error = datetime.fromisoformat(self.stats["last_error"])
                last_error_age = (current_time - last_error).total_seconds()
            
            # Calcular taxa de sucesso
            total_requests = self.stats["successful_requests"] + self.stats["failed_requests"]
            success_rate = (
                self.stats["successful_requests"] / total_requests 
                if total_requests > 0 else 0
            )
            
            # Determinar status geral
            if success_rate >= 0.8:
                overall_status = "healthy"
            elif success_rate >= 0.5:
                overall_status = "warning"
            else:
                overall_status = "critical"
            
            health_status = {
                "overall_status": overall_status,
                "running": self.running,
                "success_rate": success_rate,
                "total_requests": total_requests,
                "successful_requests": self.stats["successful_requests"],
                "failed_requests": self.stats["failed_requests"],
                "last_success_age_seconds": last_success_age,
                "last_error_age_seconds": last_error_age,
                "uptime_seconds": (
                    (current_time - datetime.fromisoformat(self.stats["started_at"])).total_seconds()
                    if self.stats["started_at"] else None
                ),
                "scrapers": {
                    name: "active" if scraper.page else "inactive"
                    for name, scraper in self.scrapers.items()
                },
                "timestamp": current_time.isoformat()
            }
            
            return health_status
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao obter status de sa√∫de: {e}")
            return {
                "overall_status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

# Fun√ß√£o principal para execu√ß√£o
async def main():
    """Fun√ß√£o principal para executar o orquestrador."""
    try:
        # Criar orquestrador
        orchestrator = OrquestradorScrapers(environment="dev")
        
        # Iniciar
        await orchestrator.start()
        
        # Executar ciclo √∫nico de teste
        print("üß™ Executando ciclo √∫nico de teste...")
        cycle_stats = await orchestrator.run_collection_cycle("test_cycle")
        
        if cycle_stats:
            print("‚úÖ Ciclo de teste conclu√≠do com sucesso!")
            print(f"üìä Dados coletados: {cycle_stats['total_data_collected']}")
        else:
            print("‚ùå Ciclo de teste falhou")
        
        # Parar orquestrador
        await orchestrator.stop()
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Execu√ß√£o interrompida pelo usu√°rio")
    except Exception as e:
        print(f"‚ùå Erro na execu√ß√£o: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # Executar orquestrador
    asyncio.run(main())
